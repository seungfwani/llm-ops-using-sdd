#!/usr/bin/env python3
"""
Hugging Face ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ë“±ë¡ ì˜ˆì œ

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” Hugging Faceì—ì„œ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  LLM Ops í”Œëž«í¼ì— ë“±ë¡í•˜ëŠ” ì˜ˆì œìž…ë‹ˆë‹¤.

ì£¼ì˜: ì‹¤ì œ ëª¨ë¸ íŒŒì¼ì€ ë§¤ìš° í´ ìˆ˜ ìžˆìœ¼ë¯€ë¡œ(ìˆ˜ GB ~ ìˆ˜ì‹­ GB), 
í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œëŠ” ë³„ë„ì˜ ì›Œí¬í”Œë¡œìš°ë¡œ ë‹¤ìš´ë¡œë“œ ë° ì—…ë¡œë“œë¥¼ ìˆ˜í–‰í•˜ëŠ” ê²ƒì„ ê¶Œìž¥í•©ë‹ˆë‹¤.
"""

import os
import sys
import requests
from pathlib import Path
from typing import Optional, Dict, Any

# Hugging Face ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© (ì„ íƒì‚¬í•­)
try:
    from huggingface_hub import snapshot_download, hf_hub_download
    HF_AVAILABLE = True
except ImportError:
    HF_AVAILABLE = False
    print("âš ï¸  huggingface_hubì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("   pip install huggingface_hub ë¡œ ì„¤ì¹˜í•˜ê±°ë‚˜, ìˆ˜ë™ìœ¼ë¡œ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”.")


class HuggingFaceModelDownloader:
    """Hugging Face ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìœ í‹¸ë¦¬í‹°"""
    
    def __init__(self, cache_dir: Optional[str] = None):
        """
        Args:
            cache_dir: ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìºì‹œ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: ~/.cache/huggingface)
        """
        self.cache_dir = cache_dir or os.path.expanduser("~/.cache/huggingface")
        Path(self.cache_dir).mkdir(parents=True, exist_ok=True)
    
    def download_model(
        self,
        model_id: str,
        local_dir: Optional[str] = None,
        token: Optional[str] = None
    ) -> str:
        """
        Hugging Faceì—ì„œ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.
        
        Args:
            model_id: Hugging Face ëª¨ë¸ ID (ì˜ˆ: "meta-llama/Llama-2-7b-chat-hf")
            local_dir: ë‹¤ìš´ë¡œë“œí•  ë¡œì»¬ ë””ë ‰í† ë¦¬ (Noneì´ë©´ ìºì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš©)
            token: Hugging Face API í† í° (gated ëª¨ë¸ì˜ ê²½ìš° í•„ìš”)
        
        Returns:
            ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ì˜ ë¡œì»¬ ê²½ë¡œ
        """
        if not HF_AVAILABLE:
            raise ImportError(
                "huggingface_hubì´ í•„ìš”í•©ë‹ˆë‹¤. "
                "pip install huggingface_hub ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”."
            )
        
        print(f"ðŸ“¥ Hugging Faceì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘: {model_id}")
        
        try:
            if local_dir:
                download_path = snapshot_download(
                    repo_id=model_id,
                    local_dir=local_dir,
                    token=token,
                    local_dir_use_symlinks=False
                )
            else:
                download_path = snapshot_download(
                    repo_id=model_id,
                    cache_dir=self.cache_dir,
                    token=token
                )
            
            print(f"âœ“ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {download_path}")
            return download_path
        
        except Exception as e:
            print(f"âœ— ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    def get_model_info(self, model_id: str) -> Dict[str, Any]:
        """
        Hugging Face ëª¨ë¸ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
        
        Args:
            model_id: Hugging Face ëª¨ë¸ ID
        
        Returns:
            ëª¨ë¸ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        try:
            from huggingface_hub import model_info
            info = model_info(model_id)
            
            return {
                "model_id": model_id,
                "author": info.author if hasattr(info, 'author') else None,
                "tags": info.tags if hasattr(info, 'tags') else [],
                "model_type": getattr(info, 'model_type', None),
                "library_name": getattr(info, 'library_name', None),
                "pipeline_tag": getattr(info, 'pipeline_tag', None),
            }
        except Exception as e:
            print(f"âš ï¸  ëª¨ë¸ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"model_id": model_id}


class CatalogClient:
    """ëª¨ë¸ ì¹´íƒˆë¡œê·¸ API í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self, base_url: str, user_id: str = "admin", user_roles: str = "admin"):
        self.base_url = base_url.rstrip('/')
        self.headers = {
            "Content-Type": "application/json",
            "X-User-Id": user_id,
            "X-User-Roles": user_roles
        }
    
    def create_model(
        self,
        name: str,
        version: str,
        model_type: str,
        owner_team: str,
        metadata: Dict[str, Any],
        storage_uri: Optional[str] = None,
        status: str = "draft"
    ) -> Dict[str, Any]:
        """ëª¨ë¸ì„ ì¹´íƒˆë¡œê·¸ì— ë“±ë¡í•©ë‹ˆë‹¤."""
        payload = {
            "name": name,
            "version": version,
            "type": model_type,
            "owner_team": owner_team,
            "metadata": metadata,
            "status": status
        }
        
        if storage_uri:
            payload["storage_uri"] = storage_uri
        
        response = requests.post(
            f"{self.base_url}/catalog/models",
            json=payload,
            headers=self.headers
        )
        response.raise_for_status()
        result = response.json()
        
        if result["status"] != "success":
            raise Exception(f"Model creation failed: {result['message']}")
        
        return result["data"]
    
    def upload_model_files(self, model_id: str, model_dir: str) -> Dict[str, Any]:
        """
        ëª¨ë¸ ë””ë ‰í† ë¦¬ì˜ íŒŒì¼ë“¤ì„ ì—…ë¡œë“œí•©ë‹ˆë‹¤.
        
        ì£¼ì˜: ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ëŒ€ìš©ëŸ‰ íŒŒì¼ì„ ìŠ¤íŠ¸ë¦¬ë° ì—…ë¡œë“œí•˜ê±°ë‚˜
        ë³„ë„ì˜ ì›Œí¬í”Œë¡œìš°ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.
        """
        import requests
        from pathlib import Path
        
        model_path = Path(model_dir)
        files_to_upload = []
        
        # ì£¼ìš” ëª¨ë¸ íŒŒì¼ë“¤ ì°¾ê¸°
        important_files = [
            "config.json",
            "tokenizer.json",
            "tokenizer_config.json",
            "model.safetensors",
            "pytorch_model.bin",
            "model.bin",
        ]
        
        for file_pattern in important_files:
            for file_path in model_path.rglob(file_pattern):
                files_to_upload.append(file_path)
        
        # ëª¨ë“  .json, .bin, .safetensors íŒŒì¼ë„ í¬í•¨
        for ext in [".json", ".bin", ".safetensors", ".txt"]:
            for file_path in model_path.rglob(f"*{ext}"):
                if file_path not in files_to_upload:
                    files_to_upload.append(file_path)
        
        print(f"\nðŸ“¤ {len(files_to_upload)}ê°œ íŒŒì¼ ì—…ë¡œë“œ ì¤€ë¹„ ì¤‘...")
        
        # ì‹¤ì œ ì—…ë¡œë“œëŠ” APIë¥¼ í†µí•´ ìˆ˜í–‰
        # ì—¬ê¸°ì„œëŠ” ì˜ˆì œë¡œ íŒŒì¼ ëª©ë¡ë§Œ ì¶œë ¥
        uploaded_files = []
        for file_path in files_to_upload[:10]:  # ì˜ˆì œ: ì²˜ìŒ 10ê°œë§Œ
            relative_path = file_path.relative_to(model_path)
            file_size = file_path.stat().st_size / (1024 * 1024)  # MB
            print(f"  - {relative_path} ({file_size:.2f} MB)")
            uploaded_files.append({
                "path": str(relative_path),
                "size_mb": file_size
            })
        
        if len(files_to_upload) > 10:
            print(f"  ... ì™¸ {len(files_to_upload) - 10}ê°œ íŒŒì¼")
        
        print("\nâš ï¸  ì‹¤ì œ íŒŒì¼ ì—…ë¡œë“œëŠ” APIë¥¼ í†µí•´ ìˆ˜í–‰í•´ì•¼ í•©ë‹ˆë‹¤:")
        print(f"   POST {self.base_url}/catalog/models/{model_id}/upload")
        print("   (multipart/form-data)")
        
        return {
            "model_id": model_id,
            "files_prepared": len(files_to_upload),
            "sample_files": uploaded_files
        }


def example_download_and_register():
    """Hugging Face ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ë“±ë¡ ì˜ˆì œ"""
    base_url = "https://dev.llm-ops.local/llm-ops/v1"
    
    # Hugging Face ëª¨ë¸ ID
    hf_model_id = "microsoft/DialoGPT-small"  # ìž‘ì€ ëª¨ë¸ë¡œ ì˜ˆì œ (ì‹¤ì œ ì‚¬ìš© ì‹œ ë” í° ëª¨ë¸ ê°€ëŠ¥)
    
    print("=" * 60)
    print("Hugging Face ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ë“±ë¡ ì˜ˆì œ")
    print("=" * 60)
    
    if not HF_AVAILABLE:
        print("\nâš ï¸  huggingface_hubì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”: pip install huggingface_hub")
        print("\n   ë˜ëŠ” ìˆ˜ë™ìœ¼ë¡œ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•œ í›„ ë“±ë¡í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.")
        return
    
    try:
        # 1. ëª¨ë¸ ì •ë³´ ì¡°íšŒ
        print(f"\n[Step 1] Hugging Face ëª¨ë¸ ì •ë³´ ì¡°íšŒ: {hf_model_id}")
        downloader = HuggingFaceModelDownloader()
        model_info = downloader.get_model_info(hf_model_id)
        print(f"  âœ“ ëª¨ë¸ ì •ë³´: {model_info}")
        
        # 2. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
        print(f"\n[Step 2] ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘...")
        download_dir = f"/tmp/hf_models/{hf_model_id.replace('/', '_')}"
        model_path = downloader.download_model(
            model_id=hf_model_id,
            local_dir=download_dir
        )
        print(f"  âœ“ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {model_path}")
        
        # 3. ëª¨ë¸ ë“±ë¡
        print(f"\n[Step 3] ëª¨ë¸ ì¹´íƒˆë¡œê·¸ì— ë“±ë¡ ì¤‘...")
        catalog_client = CatalogClient(base_url)
        
        # ëª¨ë¸ ì´ë¦„ ìƒì„± (Hugging Face IDì—ì„œ)
        model_name = hf_model_id.split("/")[-1].replace("-", "_")
        
        model = catalog_client.create_model(
            name=model_name,
            version="1.0",
            model_type="base",
            owner_team="ml-platform",
            metadata={
                "source": "huggingface",
                "huggingface_model_id": hf_model_id,
                "architecture": model_info.get("model_type", "unknown"),
                "framework": "pytorch",
                "license": "unknown",
                "description": f"Model downloaded from Hugging Face: {hf_model_id}",
                "download_path": model_path
            },
            storage_uri=f"s3://models/{model_name}/1.0/",
            status="draft"
        )
        print(f"  âœ“ ëª¨ë¸ ë“±ë¡ ì™„ë£Œ: {model['id']}")
        
        # 4. ëª¨ë¸ íŒŒì¼ ì—…ë¡œë“œ (ì˜ˆì œ - ì‹¤ì œë¡œëŠ” ë³„ë„ ì›Œí¬í”Œë¡œìš° í•„ìš”)
        print(f"\n[Step 4] ëª¨ë¸ íŒŒì¼ ì—…ë¡œë“œ ì¤€ë¹„...")
        upload_info = catalog_client.upload_model_files(model['id'], model_path)
        print(f"  âœ“ {upload_info['files_prepared']}ê°œ íŒŒì¼ ì¤€ë¹„ ì™„ë£Œ")
        
        print("\n" + "=" * 60)
        print("âœ“ ëª¨ë¸ ë“±ë¡ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ!")
        print("=" * 60)
        print(f"\në‹¤ìŒ ë‹¨ê³„:")
        print(f"1. ëª¨ë¸ íŒŒì¼ì„ S3/ê°ì²´ ìŠ¤í† ë¦¬ì§€ì— ì—…ë¡œë“œ")
        print(f"2. ëª¨ë¸ ìƒíƒœë¥¼ 'approved'ë¡œ ë³€ê²½")
        print(f"3. ì„œë¹™ ì—”ë“œí¬ì¸íŠ¸ ë°°í¬")
        
    except Exception as e:
        print(f"\nâœ— ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


def example_register_with_storage_uri():
    """ì´ë¯¸ ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ì„ storage_urië¡œ ë“±ë¡í•˜ëŠ” ì˜ˆì œ"""
    base_url = "https://dev.llm-ops.local/llm-ops/v1"
    catalog_client = CatalogClient(base_url)
    
    print("=" * 60)
    print("Hugging Face ëª¨ë¸ (storage_urië¡œ ë“±ë¡)")
    print("=" * 60)
    
    # ì´ë¯¸ S3ì— ì—…ë¡œë“œëœ ëª¨ë¸ì„ ë“±ë¡í•˜ëŠ” ê²½ìš°
    hf_model_id = "meta-llama/Llama-2-7b-chat-hf"
    model_name = "llama_2_7b_chat"
    
    print(f"\nëª¨ë¸ ë“±ë¡ ì¤‘: {model_name}")
    
    model = catalog_client.create_model(
        name=model_name,
        version="1.0",
        model_type="base",
        owner_team="ml-platform",
        metadata={
            "source": "huggingface",
            "huggingface_model_id": hf_model_id,
            "architecture": "llama",
            "parameters": "7B",
            "framework": "pytorch",
            "license": "llama2",
            "description": f"Llama 2 7B Chat model from Hugging Face"
        },
        storage_uri="s3://models/llama_2_7b_chat/1.0/",  # ì´ë¯¸ ì—…ë¡œë“œëœ ëª¨ë¸ ê²½ë¡œ
        status="draft"
    )
    
    print(f"âœ“ ëª¨ë¸ ë“±ë¡ ì™„ë£Œ: {model['id']}")
    print(f"  Storage URI: {model.get('storage_uri', 'N/A')}")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        example_name = sys.argv[1]
        if example_name == "download":
            example_download_and_register()
        elif example_name == "register":
            example_register_with_storage_uri()
        else:
            print(f"Unknown example: {example_name}")
            print("Available examples: download, register")
    else:
        print("Usage: python download_and_register_hf_model.py <example_name>")
        print("\nAvailable examples:")
        print("  download - Download model from Hugging Face and register")
        print("  register - Register model with existing storage_uri")
        print("\nExample: python download_and_register_hf_model.py download")
        print("\nNote: Install huggingface_hub first:")
        print("  pip install huggingface_hub")

