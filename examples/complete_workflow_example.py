#!/usr/bin/env python3
"""
LLM Ops Platform ì „ì²´ ì›Œí¬í”Œë¡œìš° ì˜ˆì œ

ë°ì´í„°ì…‹ ë“±ë¡ë¶€í„° ëª¨ë¸ ì„œë¹™ ë° ì±„íŒ… í…ŒìŠ¤íŠ¸ê¹Œì§€ì˜ ì „ì²´ ê³¼ì •ì„ ë³´ì—¬ì£¼ëŠ” ì˜ˆì œì…ë‹ˆë‹¤.

ì›Œí¬í”Œë¡œìš°:
1. ë°ì´í„°ì…‹ ë“±ë¡ ë° ì—…ë¡œë“œ
2. Base ëª¨ë¸ ë“±ë¡ (ë˜ëŠ” Hugging Face ëª¨ë¸)
3. í•™ìŠµ ì‘ì—… ì œì¶œ (ì„ íƒì‚¬í•­)
4. ëª¨ë¸ ìŠ¹ì¸
5. ì„œë¹™ ì—”ë“œí¬ì¸íŠ¸ ë°°í¬ (DeploymentSpec í¬í•¨)
6. ì±„íŒ… í…ŒìŠ¤íŠ¸

ì‚¬ìš© ë°©ë²•:
    python examples/complete_workflow_example.py

í™˜ê²½ ë³€ìˆ˜:
    LLM_OPS_API_BASE_URL: API ê¸°ë³¸ URL (ê¸°ë³¸ê°’: http://localhost:8000/llm-ops/v1)
    LLM_OPS_USER_ID: ì‚¬ìš©ì ID (ê¸°ë³¸ê°’: admin)
    LLM_OPS_USER_ROLES: ì‚¬ìš©ì ì—­í•  (ê¸°ë³¸ê°’: admin,llm-ops-user)
                        ì£¼ì˜: llm-ops-user ì—­í• ì´ í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤ (governance ë¯¸ë“¤ì›¨ì–´ ìš”êµ¬ì‚¬í•­)
    USE_GPU: GPU ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: false, ë¡œì»¬ ê°œë°œ í™˜ê²½ì—ì„œëŠ” CPU ì‚¬ìš©)
             GPUë¥¼ ì‚¬ìš©í•˜ë ¤ë©´: export USE_GPU=true
"""

import os
import sys
import time
import requests
import json
from pathlib import Path
from typing import Optional, Dict, List, Any

# ì˜ˆì œ ë°ì´í„°ì…‹ ê²½ë¡œ
EXAMPLE_DATASET_PATH = Path(__file__).parent / "datasets" / "customer-support-sample.csv"


class CatalogClient:
    """ì¹´íƒˆë¡œê·¸ API í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self, base_url: str, user_id: str = "admin", user_roles: str = "admin"):
        self.base_url = base_url.rstrip('/')
        self.headers = {
            "Content-Type": "application/json",
            "X-User-Id": user_id,
            "X-User-Roles": user_roles
        }
    
    def list_datasets(self) -> List[Dict[str, Any]]:
        """ë°ì´í„°ì…‹ ëª©ë¡ ì¡°íšŒ"""
        response = requests.get(
            f"{self.base_url}/catalog/datasets",
            headers=self.headers
        )
        response.raise_for_status()
        result = response.json()
        if result["status"] != "success":
            raise Exception(f"Dataset list failed: {result['message']}")
        return result.get("data", [])
    
    def get_dataset_by_name_version(self, name: str, version: str) -> Optional[Dict[str, Any]]:
        """ì´ë¦„ê³¼ ë²„ì „ìœ¼ë¡œ ë°ì´í„°ì…‹ ì¡°íšŒ"""
        datasets = self.list_datasets()
        for dataset in datasets:
            if dataset.get("name") == name and dataset.get("version") == version:
                return dataset
        return None
    
    def create_dataset(
        self,
        name: str,
        version: str,
        owner_team: str = "ml-platform",
        dataset_type: str = "sft_pair",
        storage_uri: Optional[str] = None,
        reuse_existing: bool = True
    ) -> Dict[str, Any]:
        """ë°ì´í„°ì…‹ ìƒì„± (ê¸°ì¡´ ë°ì´í„°ì…‹ì´ ìˆìœ¼ë©´ ì¬ì‚¬ìš© ê°€ëŠ¥)"""
        # ê¸°ì¡´ ë°ì´í„°ì…‹ í™•ì¸
        if reuse_existing:
            existing = self.get_dataset_by_name_version(name, version)
            if existing:
                print(f"  â„¹ï¸  ê¸°ì¡´ ë°ì´í„°ì…‹ ë°œê²¬: {existing['id']} (ì¬ì‚¬ìš©)")
                return existing
        
        # storage_uriê°€ ì œê³µë˜ì§€ ì•Šìœ¼ë©´ ìë™ ìƒì„±
        if storage_uri is None:
            storage_uri = f"s3://datasets/{name}/{version}/"
        
        payload = {
            "name": name,
            "version": version,
            "owner_team": owner_team,  # APIëŠ” snake_case ì‚¬ìš©
            "type": dataset_type,
            "storage_uri": storage_uri  # APIëŠ” snake_case ì‚¬ìš©
        }
        response = requests.post(
            f"{self.base_url}/catalog/datasets",
            json=payload,
            headers=self.headers
        )
        response.raise_for_status()
        result = response.json()
        if result["status"] != "success":
            raise Exception(f"Dataset creation failed: {result['message']}")
        return result["data"]
    
    def upload_dataset(self, dataset_id: str, file_path: Path) -> Dict[str, Any]:
        """ë°ì´í„°ì…‹ íŒŒì¼ ì—…ë¡œë“œ"""
        with open(file_path, 'rb') as f:
            files = {'files': (file_path.name, f, 'text/csv')}
            headers = {k: v for k, v in self.headers.items() if k != "Content-Type"}
            response = requests.post(
                f"{self.base_url}/catalog/datasets/{dataset_id}/upload",
                files=files,
                headers=headers
            )
        response.raise_for_status()
        result = response.json()
        if result["status"] != "success":
            raise Exception(f"Dataset upload failed: {result['message']}")
        return result["data"]
    
    def list_models(self) -> List[Dict[str, Any]]:
        """ëª¨ë¸ ëª©ë¡ ì¡°íšŒ"""
        response = requests.get(
            f"{self.base_url}/catalog/models",
            headers=self.headers
        )
        response.raise_for_status()
        result = response.json()
        if result["status"] != "success":
            raise Exception(f"Model list failed: {result['message']}")
        return result.get("data", [])
    
    def get_model_by_name_version(self, name: str, version: str) -> Optional[Dict[str, Any]]:
        """ì´ë¦„ê³¼ ë²„ì „ìœ¼ë¡œ ëª¨ë¸ ì¡°íšŒ"""
        models = self.list_models()
        for model in models:
            if model.get("name") == name and model.get("version") == version:
                return model
        return None
    
    def create_model(
        self,
        name: str,
        version: str,
        model_type: str,
        model_family: str,
        owner_team: str = "ml-platform",
        metadata: Optional[Dict] = None,
        storage_uri: Optional[str] = None,
        status: str = "draft",
        reuse_existing: bool = True
    ) -> Dict[str, Any]:
        """ëª¨ë¸ ë“±ë¡ (ê¸°ì¡´ ëª¨ë¸ì´ ìˆìœ¼ë©´ ì¬ì‚¬ìš© ê°€ëŠ¥)"""
        # ê¸°ì¡´ ëª¨ë¸ í™•ì¸
        if reuse_existing:
            existing = self.get_model_by_name_version(name, version)
            if existing:
                print(f"  â„¹ï¸  ê¸°ì¡´ ëª¨ë¸ ë°œê²¬: {existing['id']} (ì¬ì‚¬ìš©)")
                return existing
        
        payload = {
            "name": name,
            "version": version,
            "type": model_type,
            "model_family": model_family,
            "owner_team": owner_team,
            "metadata": metadata or {},
            "status": status
        }
        if storage_uri:
            payload["storage_uri"] = storage_uri
        
        response = requests.post(
            f"{self.base_url}/catalog/models",
            json=payload,
            headers=self.headers
        )
        response.raise_for_status()
        result = response.json()
        if result["status"] != "success":
            raise Exception(f"Model creation failed: {result['message']}")
        return result["data"]
    
    def update_model_status(self, model_id: str, status: str) -> Dict[str, Any]:
        """ëª¨ë¸ ìƒíƒœ ì—…ë°ì´íŠ¸"""
        response = requests.patch(
            f"{self.base_url}/catalog/models/{model_id}/status",
            params={"status": status},
            headers=self.headers
        )
        response.raise_for_status()
        result = response.json()
        if result["status"] != "success":
            raise Exception(f"Model status update failed: {result['message']}")
        return result["data"]
    
    def get_model(self, model_id: str) -> Dict[str, Any]:
        """ëª¨ë¸ ì¡°íšŒ"""
        response = requests.get(
            f"{self.base_url}/catalog/models/{model_id}",
            headers=self.headers
        )
        response.raise_for_status()
        result = response.json()
        if result["status"] != "success":
            raise Exception(f"Model retrieval failed: {result['message']}")
        return result["data"]
    
    def update_dataset_status(self, dataset_id: str, status: str) -> Dict[str, Any]:
        """ë°ì´í„°ì…‹ ìƒíƒœ ì—…ë°ì´íŠ¸"""
        response = requests.patch(
            f"{self.base_url}/catalog/datasets/{dataset_id}/status",
            params={"status": status},
            headers=self.headers
        )
        response.raise_for_status()
        result = response.json()
        if result["status"] != "success":
            raise Exception(f"Dataset status update failed: {result['message']}")
        return result["data"]
    
    def upload_model_files(self, model_id: str, file_paths: List[Path]) -> Dict[str, Any]:
        """ëª¨ë¸ íŒŒì¼ ì—…ë¡œë“œ"""
        files = []
        file_handles = []
        
        try:
            for file_path in file_paths:
                if file_path.exists():
                    f = open(file_path, 'rb')
                    file_handles.append(f)
                    files.append(('files', (file_path.name, f, 'application/octet-stream')))
            
            if not files:
                raise Exception("No valid files to upload")
            
            headers = {k: v for k, v in self.headers.items() if k != "Content-Type"}
            response = requests.post(
                f"{self.base_url}/catalog/models/{model_id}/upload",
                files=files,
                headers=headers
            )
            
            response.raise_for_status()
            result = response.json()
            if result["status"] != "success":
                raise Exception(f"Model file upload failed: {result['message']}")
            return result["data"]
        finally:
            # íŒŒì¼ í•¸ë“¤ ë‹«ê¸°
            for f in file_handles:
                try:
                    f.close()
                except Exception:
                    pass


class TrainingClient:
    """í•™ìŠµ API í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self, base_url: str, user_id: str = "admin", user_roles: str = "admin"):
        self.base_url = base_url.rstrip('/')
        self.headers = {
            "Content-Type": "application/json",
            "X-User-Id": user_id,
            "X-User-Roles": user_roles
        }
    
    def submit_job(
        self,
        model_id: Optional[str],
        dataset_id: str,
        job_type: str,
        use_gpu: bool = False,
        resource_profile: Optional[Dict] = None,
        train_job_spec: Optional[Dict] = None
    ) -> Dict[str, Any]:
        """í•™ìŠµ ì‘ì—… ì œì¶œ"""
        if resource_profile is None:
            if use_gpu:
                resource_profile = {
                    "gpuCount": 1,
                    "gpuType": "nvidia-tesla-v100",
                    "maxDuration": 60
                }
            else:
                resource_profile = {
                    "cpuCores": 4,
                    "memory": "8Gi",
                    "maxDuration": 60
                }
        
        payload = {
            "datasetId": dataset_id,
            "jobType": job_type,
            "useGpu": use_gpu,
            "resourceProfile": resource_profile
        }
        
        if model_id:
            payload["modelId"] = model_id
        
        if train_job_spec:
            payload["trainJobSpec"] = train_job_spec
        
        response = requests.post(
            f"{self.base_url}/training/jobs",
            json=payload,
            headers=self.headers
        )
        response.raise_for_status()
        result = response.json()
        if result["status"] != "success":
            raise Exception(f"Training job submission failed: {result['message']}")
        return result["data"]
    
    def get_job(self, job_id: str) -> Dict[str, Any]:
        """í•™ìŠµ ì‘ì—… ì¡°íšŒ"""
        response = requests.get(
            f"{self.base_url}/training/jobs/{job_id}",
            headers=self.headers
        )
        response.raise_for_status()
        result = response.json()
        if result["status"] != "success":
            raise Exception(f"Training job retrieval failed: {result['message']}")
        return result["data"]


class ServingClient:
    """ì„œë¹™ API í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self, base_url: str, user_id: str = "admin", user_roles: str = "admin"):
        self.base_url = base_url.rstrip('/')
        self.headers = {
            "Content-Type": "application/json",
            "X-User-Id": user_id,
            "X-User-Roles": user_roles
        }
    
    def deploy_endpoint(
        self,
        model_id: str,
        environment: str,
        route: str,
        min_replicas: int = 1,
        max_replicas: int = 3,
        deployment_spec: Optional[Dict] = None,
        use_gpu: bool = False
    ) -> Dict[str, Any]:
        """ì„œë¹™ ì—”ë“œí¬ì¸íŠ¸ ë°°í¬"""
        payload = {
            "modelId": model_id,
            "environment": environment,
            "route": route,
            "minReplicas": min_replicas,
            "maxReplicas": max_replicas,
            "useGpu": use_gpu
        }
        
        if deployment_spec:
            payload["deploymentSpec"] = deployment_spec
        
        response = requests.post(
            f"{self.base_url}/serving/endpoints",
            json=payload,
            headers=self.headers
        )
        response.raise_for_status()
        result = response.json()
        if result["status"] != "success":
            raise Exception(f"Endpoint deployment failed: {result['message']}")
        return result["data"]
    
    def list_endpoints(
        self,
        environment: Optional[str] = None,
        model_id: Optional[str] = None,
        status: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """ì—”ë“œí¬ì¸íŠ¸ ëª©ë¡ ì¡°íšŒ"""
        params = {}
        if environment:
            params["environment"] = environment
        if model_id:
            params["modelId"] = model_id
        if status:
            params["status"] = status
        
        response = requests.get(
            f"{self.base_url}/serving/endpoints",
            params=params,
            headers=self.headers
        )
        response.raise_for_status()
        result = response.json()
        if result["status"] != "success":
            raise Exception(f"Endpoint list failed: {result['message']}")
        return result.get("data", [])
    
    def get_endpoint(self, endpoint_id: str) -> Dict[str, Any]:
        """ì—”ë“œí¬ì¸íŠ¸ ì¡°íšŒ"""
        response = requests.get(
            f"{self.base_url}/serving/endpoints/{endpoint_id}",
            headers=self.headers
        )
        response.raise_for_status()
        result = response.json()
        if result["status"] != "success":
            raise Exception(f"Endpoint retrieval failed: {result['message']}")
        return result["data"]
    
    def delete_endpoint(self, endpoint_id: str) -> Dict[str, Any]:
        """ì—”ë“œí¬ì¸íŠ¸ ì‚­ì œ"""
        response = requests.delete(
            f"{self.base_url}/serving/endpoints/{endpoint_id}",
            headers=self.headers
        )
        response.raise_for_status()
        result = response.json()
        if result["status"] != "success":
            raise Exception(f"Endpoint deletion failed: {result['message']}")
        return result.get("data", {})
    
    def get_endpoint_by_route(self, route: str, environment: str) -> Optional[Dict[str, Any]]:
        """Routeì™€ environmentë¡œ ì—”ë“œí¬ì¸íŠ¸ ì¡°íšŒ"""
        endpoints = self.list_endpoints(environment=environment)
        for endpoint in endpoints:
            if endpoint.get("route") == route:
                return endpoint
        return None
    
    def wait_for_healthy(self, endpoint_id: str, timeout: int = 300) -> bool:
        """ì—”ë“œí¬ì¸íŠ¸ê°€ healthy ìƒíƒœê°€ ë  ë•Œê¹Œì§€ ëŒ€ê¸°"""
        start_time = time.time()
        while time.time() - start_time < timeout:
            endpoint = self.get_endpoint(endpoint_id)
            status = endpoint.get("status")
            print(f"  ì—”ë“œí¬ì¸íŠ¸ ìƒíƒœ: {status}")
            if status == "healthy":
                return True
            elif status == "failed":
                raise Exception("Endpoint deployment failed")
            time.sleep(5)
        raise Exception(f"Endpoint did not become healthy within {timeout} seconds")
    
    def chat_completion(
        self,
        route_name: str,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: int = 500
    ) -> Dict[str, Any]:
        """ì±„íŒ… ì™„ì„± API í˜¸ì¶œ"""
        payload = {
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens
        }
        response = requests.post(
            f"{self.base_url}/serve/{route_name}/chat",
            json=payload,
            headers=self.headers
        )
        response.raise_for_status()
        result = response.json()
        return result


def print_section(title: str):
    """ì„¹ì…˜ ì œëª© ì¶œë ¥"""
    print("\n" + "=" * 60)
    print(f"  {title}")
    print("=" * 60)


def complete_workflow_example():
    """ì „ì²´ ì›Œí¬í”Œë¡œìš° ì˜ˆì œ ì‹¤í–‰"""
    base_url = os.getenv("LLM_OPS_API_BASE_URL", "http://localhost:8000/llm-ops/v1")
    user_id = os.getenv("LLM_OPS_USER_ID", "admin")
    # ê¸°ë³¸ ì—­í• ì— llm-ops-userë¥¼ í¬í•¨ (governance ë¯¸ë“¤ì›¨ì–´ ìš”êµ¬ì‚¬í•­)
    user_roles = os.getenv("LLM_OPS_USER_ROLES", "admin,llm-ops-user")
    
    print_section("LLM Ops Platform ì „ì²´ ì›Œí¬í”Œë¡œìš° ì˜ˆì œ")
    print(f"\nAPI URL: {base_url}")
    print(f"User ID: {user_id}")
    print(f"User Roles: {user_roles}")
    
    # CPU/GPU ëª¨ë“œ í™•ì¸
    use_gpu_env = os.getenv("USE_GPU", "false").lower() == "true"
    print(f"ë¦¬ì†ŒìŠ¤ ëª¨ë“œ: {'GPU' if use_gpu_env else 'CPU (ë¡œì»¬ ê°œë°œ ëª¨ë“œ)'}")
    if not use_gpu_env:
        print(f"  ğŸ’¡ GPUë¥¼ ì‚¬ìš©í•˜ë ¤ë©´: export USE_GPU=true")
    
    catalog_client = CatalogClient(base_url, user_id, user_roles)
    training_client = TrainingClient(base_url, user_id, user_roles)
    serving_client = ServingClient(base_url, user_id, user_roles)
    
    try:
        # Step 1: ë°ì´í„°ì…‹ ë“±ë¡ ë° ì—…ë¡œë“œ
        print_section("Step 1: ë°ì´í„°ì…‹ ë“±ë¡ ë° ì—…ë¡œë“œ")
        dataset = catalog_client.create_dataset(
            name="customer-support-dataset",
            version="v1.0",
            owner_team="ml-platform",
            dataset_type="sft_pair"  # SFT fine-tuningìš© ë°ì´í„°ì…‹ íƒ€ì…
        )
        dataset_id = dataset["id"]
        print(f"  âœ“ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ: {dataset_id}")
        
        if EXAMPLE_DATASET_PATH.exists():
            upload_result = catalog_client.upload_dataset(dataset_id, EXAMPLE_DATASET_PATH)
            print(f"  âœ“ ë°ì´í„°ì…‹ íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ: {upload_result.get('files_uploaded', 0)}ê°œ íŒŒì¼")
        else:
            print(f"  âš ï¸  ì˜ˆì œ ë°ì´í„°ì…‹ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {EXAMPLE_DATASET_PATH}")
            print(f"     ë°ì´í„°ì…‹ íŒŒì¼ì„ ìˆ˜ë™ìœ¼ë¡œ ì—…ë¡œë“œí•˜ì„¸ìš”.")
        
        # Step 1.5: ë°ì´í„°ì…‹ ìŠ¹ì¸ (í•™ìŠµ ì‘ì—…ì„ ìœ„í•´ í•„ìš”)
        print_section("Step 1.5: ë°ì´í„°ì…‹ ìŠ¹ì¸")
        try:
            approved_dataset = catalog_client.update_dataset_status(dataset_id, "approved")
            print(f"  âœ“ ë°ì´í„°ì…‹ ìŠ¹ì¸ ì™„ë£Œ: {approved_dataset.get('status', 'approved')}")
        except Exception as e:
            print(f"  âš ï¸  ë°ì´í„°ì…‹ ìŠ¹ì¸ ì‹¤íŒ¨: {e}")
            print(f"     í•™ìŠµ ì‘ì—…ì„ ê±´ë„ˆë›°ê±°ë‚˜ ìˆ˜ë™ìœ¼ë¡œ ìŠ¹ì¸í•˜ì„¸ìš”.")
        
        # Step 2: Base ëª¨ë¸ ë“±ë¡
        print_section("Step 2: Base ëª¨ë¸ ë“±ë¡")
        model = catalog_client.create_model(
            name="example-base-model",
            version="1.0",
            model_type="base",
            model_family="llama",
            owner_team="ml-platform",
            metadata={
                "architecture": "transformer",
                "parameters": "7B",
                "framework": "pytorch",
                "description": "Example base model for workflow demonstration"
            },
            storage_uri="s3://models/example-base-model/1.0/",
            status="draft"
        )
        model_id = model["id"]
        print(f"  âœ“ ëª¨ë¸ ë“±ë¡ ì™„ë£Œ: {model_id}")
        
        # Step 2.5: ëª¨ë¸ íŒŒì¼ ì—…ë¡œë“œ (storage_uriê°€ ì—†ì„ ê²½ìš°)
        # ì°¸ê³ : ì‹¤ì œ ëª¨ë¸ íŒŒì¼ì´ ì—†ìœ¼ë©´ ì´ ë‹¨ê³„ë¥¼ ê±´ë„ˆë›°ê³ , 
        # ì„œë¹™ ì‹œ storage_uriê°€ ì—†ìœ¼ë©´ ì™¸ë¶€ ëª¨ë¸ë¡œ ì²˜ë¦¬í•˜ê±°ë‚˜ ê²½ê³ ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
        if not model.get("storage_uri"):
            print(f"  âš ï¸  ëª¨ë¸ì— storage_uriê°€ ì—†ìŠµë‹ˆë‹¤.")
            print(f"     ëª¨ë¸ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê±°ë‚˜ ì™¸ë¶€ ëª¨ë¸ë¡œ ì„¤ì •í•˜ì„¸ìš”.")
            print(f"     ì˜ˆ: catalog_client.upload_model_files(model_id, [Path('model.bin')])")
        else:
            print(f"  âœ“ ëª¨ë¸ storage_uri: {model.get('storage_uri')}")
        
        # Step 3: ëª¨ë¸ ìŠ¹ì¸
        print_section("Step 3: ëª¨ë¸ ìŠ¹ì¸")
        approved_model = catalog_client.update_model_status(model_id, "approved")
        print(f"  âœ“ ëª¨ë¸ ìŠ¹ì¸ ì™„ë£Œ: {approved_model['status']}")
        
        # Step 4: í•™ìŠµ ì‘ì—… ì œì¶œ (ì„ íƒì‚¬í•­)
        print_section("Step 4: í•™ìŠµ ì‘ì—… ì œì¶œ (ì„ íƒì‚¬í•­)")
        print("  í•™ìŠµ ì‘ì—…ì„ ì œì¶œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ", end="")
        submit_training = input().strip().lower() == 'y'
        
        training_job_id = None
        if submit_training:
            try:
                # CPU-only í•™ìŠµ (ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©)
                train_job_spec = {
                    "model_ref": f"{model['name']}-{model['version']}",
                    "model_family": "llama",
                    "job_type": "SFT",
                    "dataset_ref": f"{dataset['name']}-{dataset['version']}",
                    "dataset_type": "instruction",
                    "resources": {
                        "gpus": 0  # CPU-only
                    },
                    "hyperparameters": {
                        "learning_rate": 2e-5,
                        "batch_size": 4,
                        "num_epochs": 1
                    },
                    "use_gpu": False
                }
                
                training_job = training_client.submit_job(
                    model_id=model_id,
                    dataset_id=dataset_id,
                    job_type="finetune",
                    use_gpu=False,
                    train_job_spec=train_job_spec
                )
                training_job_id = training_job["id"]
                print(f"  âœ“ í•™ìŠµ ì‘ì—… ì œì¶œ ì™„ë£Œ: {training_job_id}")
                print(f"     ìƒíƒœ: {training_job['status']}")
                print(f"     ì°¸ê³ : í•™ìŠµ ì‘ì—…ì€ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
            except Exception as e:
                print(f"  âš ï¸  í•™ìŠµ ì‘ì—… ì œì¶œ ì‹¤íŒ¨: {e}")
                print(f"     ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤...")
        else:
            print("  í•™ìŠµ ì‘ì—… ì œì¶œì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        
        # Step 5: ì„œë¹™ ì—”ë“œí¬ì¸íŠ¸ ë°°í¬
        print_section("Step 5: ì„œë¹™ ì—”ë“œí¬ì¸íŠ¸ ë°°í¬")
        
        # ëª¨ë¸ ì •ë³´ ë‹¤ì‹œ ì¡°íšŒí•˜ì—¬ storage_uri í™•ì¸
        model_info = catalog_client.get_model(model_id)
        if not model_info.get("storage_uri"):
            print(f"  âš ï¸  ê²½ê³ : ëª¨ë¸ì— storage_uriê°€ ì—†ìŠµë‹ˆë‹¤.")
            print(f"     ì„œë¹™ ë°°í¬ê°€ ì‹¤íŒ¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            print(f"     ëª¨ë¸ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê±°ë‚˜ ì™¸ë¶€ ëª¨ë¸ë¡œ ì„¤ì •í•˜ì„¸ìš”.")
            print(f"     ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤...")
        
        # DeploymentSpec ìƒì„±
        # ë¡œì»¬ ê°œë°œ í™˜ê²½ì„ ìœ„í•´ ê¸°ë³¸ì ìœ¼ë¡œ CPU ì‚¬ìš© (USE_GPU í™˜ê²½ ë³€ìˆ˜ë¡œ GPU í™œì„±í™” ê°€ëŠ¥)
        use_gpu = os.getenv("USE_GPU", "false").lower() == "true"
        
        deployment_spec = {
            "model_ref": f"{model['name']}-{model['version']}",
            "model_family": "llama",
            "job_type": "SFT",
            "serve_target": "GENERATION",
            "resources": {
                "gpus": 1 if use_gpu else 0,
            },
            "runtime": {
                "max_concurrent_requests": 256,
                "max_input_tokens": 4096,
                "max_output_tokens": 1024
            },
            "use_gpu": use_gpu
        }
        
        # CPU ëª¨ë“œì¼ ë•ŒëŠ” GPU ë©”ëª¨ë¦¬ ì„¤ì • ì œê±°
        if use_gpu:
            deployment_spec["resources"]["gpu_memory_gb"] = 80
        route = "/llm-ops/v1/serve/example-model"
        environment = "dev"
        
        # ê¸°ì¡´ ì—”ë“œí¬ì¸íŠ¸ í™•ì¸
        existing_endpoint = serving_client.get_endpoint_by_route(route, environment)
        if existing_endpoint:
            print(f"  â„¹ï¸  ê¸°ì¡´ ì—”ë“œí¬ì¸íŠ¸ ë°œê²¬: {existing_endpoint['id']}")
            print(f"     Route: {route}")
            print(f"     ìƒíƒœ: {existing_endpoint.get('status', 'unknown')}")
            
            # ê¸°ì¡´ ì—”ë“œí¬ì¸íŠ¸ ì¬ì‚¬ìš© ë˜ëŠ” ì‚­ì œ í›„ ì¬ë°°í¬
            reuse = input("  ê¸°ì¡´ ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì¬ì‚¬ìš©í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n, ê¸°ë³¸ê°’: y): ").strip().lower()
            if reuse == 'n':
                print(f"  ê¸°ì¡´ ì—”ë“œí¬ì¸íŠ¸ ì‚­ì œ ì¤‘...")
                serving_client.delete_endpoint(existing_endpoint['id'])
                print(f"  âœ“ ê¸°ì¡´ ì—”ë“œí¬ì¸íŠ¸ ì‚­ì œ ì™„ë£Œ")
                
                # ìƒˆ ì—”ë“œí¬ì¸íŠ¸ ë°°í¬
                endpoint = serving_client.deploy_endpoint(
                    model_id=model_id,
                    environment=environment,
                    route=route,
                    min_replicas=1,
                    max_replicas=3,
                    deployment_spec=deployment_spec,
                    use_gpu=use_gpu
                )
            else:
                # ê¸°ì¡´ ì—”ë“œí¬ì¸íŠ¸ ì¬ì‚¬ìš©
                endpoint = existing_endpoint
                print(f"  âœ“ ê¸°ì¡´ ì—”ë“œí¬ì¸íŠ¸ ì¬ì‚¬ìš©: {endpoint['id']}")
        else:
            # ìƒˆ ì—”ë“œí¬ì¸íŠ¸ ë°°í¬
            endpoint = serving_client.deploy_endpoint(
                model_id=model_id,
                environment=environment,
                route=route,
                min_replicas=1,
                max_replicas=3,
                deployment_spec=deployment_spec,
                use_gpu=use_gpu
            )
        endpoint_id = endpoint["id"]
        print(f"  âœ“ ì„œë¹™ ì—”ë“œí¬ì¸íŠ¸ ë°°í¬ ì™„ë£Œ: {endpoint_id}")
        print(f"     Route: {route}")
        print(f"     ë¦¬ì†ŒìŠ¤: {'GPU' if use_gpu else 'CPU (ë¡œì»¬ ê°œë°œ ëª¨ë“œ)'}")
        print(f"     ìƒíƒœ: {endpoint['status']}")
        
        # Step 6: ì—”ë“œí¬ì¸íŠ¸ê°€ healthy ìƒíƒœê°€ ë  ë•Œê¹Œì§€ ëŒ€ê¸°
        print_section("Step 6: ì—”ë“œí¬ì¸íŠ¸ ë°°í¬ ëŒ€ê¸°")
        print("  ì—”ë“œí¬ì¸íŠ¸ê°€ healthy ìƒíƒœê°€ ë  ë•Œê¹Œì§€ ëŒ€ê¸° ì¤‘...")
        try:
            serving_client.wait_for_healthy(endpoint_id, timeout=300)
            print("  âœ“ ì—”ë“œí¬ì¸íŠ¸ê°€ healthy ìƒíƒœì…ë‹ˆë‹¤!")
        except Exception as e:
            print(f"  âš ï¸  ì—”ë“œí¬ì¸íŠ¸ ëŒ€ê¸° ì¤‘ ì˜¤ë¥˜: {e}")
            print(f"     ìˆ˜ë™ìœ¼ë¡œ ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        
        # Step 7: ì±„íŒ… í…ŒìŠ¤íŠ¸
        print_section("Step 7: ì±„íŒ… í…ŒìŠ¤íŠ¸")
        route_name = route.split("/")[-1]  # "example-model"
        
        messages = [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "Hello! Can you help me?"}
        ]
        
        print(f"  ì±„íŒ… ìš”ì²­ ì „ì†¡ ì¤‘...")
        print(f"  ë©”ì‹œì§€: {messages[-1]['content']}")
        
        try:
            response = serving_client.chat_completion(
                route_name=route_name,
                messages=messages,
                temperature=0.7,
                max_tokens=100
            )
            
            if response.get("status") == "success" and response.get("data"):
                choice = response["data"]["choices"][0]
                assistant_message = choice["message"]["content"]
                print(f"  âœ“ ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ!")
                print(f"  ì‘ë‹µ: {assistant_message}")
                
                if response["data"].get("usage"):
                    usage = response["data"]["usage"]
                    print(f"  í† í° ì‚¬ìš©ëŸ‰:")
                    print(f"    - Prompt: {usage.get('prompt_tokens', 0)}")
                    print(f"    - Completion: {usage.get('completion_tokens', 0)}")
                    print(f"    - Total: {usage.get('total_tokens', 0)}")
            else:
                print(f"  âš ï¸  ì±„íŒ… ì‘ë‹µ ì‹¤íŒ¨: {response.get('message', 'Unknown error')}")
        except Exception as e:
            print(f"  âš ï¸  ì±„íŒ… í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            print(f"     ì—”ë“œí¬ì¸íŠ¸ê°€ ì•„ì§ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ëª¨ë¸ì´ ë°°í¬ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        # ìš”ì•½
        print_section("ì›Œí¬í”Œë¡œìš° ì™„ë£Œ ìš”ì•½")
        print(f"  ë°ì´í„°ì…‹ ID: {dataset_id}")
        print(f"  ëª¨ë¸ ID: {model_id}")
        if training_job_id:
            print(f"  í•™ìŠµ ì‘ì—… ID: {training_job_id}")
        print(f"  ì„œë¹™ ì—”ë“œí¬ì¸íŠ¸ ID: {endpoint_id}")
        print(f"  ì„œë¹™ Route: {route}")
        print(f"\n  ë‹¤ìŒ ë‹¨ê³„:")
        print(f"    1. UIì—ì„œ ì—”ë“œí¬ì¸íŠ¸ ìƒíƒœ í™•ì¸: /serving/endpoints/{endpoint_id}")
        print(f"    2. ì±„íŒ… í…ŒìŠ¤íŠ¸: /serving/chat/{endpoint_id}")
        print(f"    3. APIë¡œ ì±„íŒ…: POST {base_url}/serve/{route_name}/chat")
        
    except Exception as e:
        print(f"\nâœ— ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    complete_workflow_example()

