# =============================================================================
# LLM Ops Platform - Environment Configuration Example
# =============================================================================
# Copy this file to .env and adjust values for your environment:
#   cd backend
#   cp env.example .env
#
# IMPORTANT: 
#   - Do not commit .env file to git (it's in .gitignore)
#   - All boolean values should be lowercase: true or false
#   - Empty values for optional fields can be left as empty string or commented out

# =============================================================================
# Database Configuration
# =============================================================================
# PostgreSQL connection URL
# Format: postgresql+psycopg://user:password@host:port/database
# Local (with port-forward): postgresql+psycopg://llmops:password@localhost:5432/llmops
# Kubernetes: postgresql+psycopg://llmops:password@postgresql.llm-ops-dev.svc.cluster.local:5432/llmops
DATABASE_URL=postgresql+psycopg://llmops:password@localhost:5432/llmops

# Enable SQLAlchemy query logging (for debugging)
# Values: true, false
SQLALCHEMY_ECHO=false

# =============================================================================
# Redis Configuration
# =============================================================================
# Redis connection URL
# Local (with port-forward): redis://localhost:6379/0
# Kubernetes: redis://redis.llm-ops-dev.svc.cluster.local:6379/0
REDIS_URL=redis://localhost:6379/0

# =============================================================================
# Object Storage Configuration (MinIO/S3)
# =============================================================================
# Object storage endpoint URL
# Local (with port-forward): http://localhost:9000
# Kubernetes: http://minio.llm-ops-dev.svc.cluster.local:9000
# Production S3: https://s3.amazonaws.com
OBJECT_STORE_ENDPOINT=http://localhost:9000

# Object storage access credentials
OBJECT_STORE_ACCESS_KEY=llmops
OBJECT_STORE_SECRET_KEY=llmops-secret

# Use HTTPS for object storage (set to true for production S3)
# Values: true, false
OBJECT_STORE_SECURE=false

# =============================================================================
# Application Configuration
# =============================================================================
# Prometheus metrics namespace
PROMETHEUS_NAMESPACE=llm_ops

# Default required role for API access
DEFAULT_REQUIRED_ROLE=llm-ops-user

# =============================================================================
# Kubernetes Configuration
# =============================================================================
# Path to kubeconfig file (leave empty or unset to use default or in-cluster config)
# Local: ~/.kube/config or /path/to/kubeconfig
# In-cluster: leave this line commented out or empty (auto-detected)
# Uncomment and set if you need a specific kubeconfig file:
# KUBECONFIG_PATH=~/.kube/config

# =============================================================================
# Serving Configuration
# =============================================================================
# Model serving runtime image (vLLM, TGI, etc.)
# Options:
#   - python:3.11-slim (lightweight, for local testing)
#   - ghcr.io/vllm/vllm:latest (GitHub Container Registry - recommended)
#   - ghcr.io/vllm/vllm:0.6.0 (specific version)
#   - huggingface/text-generation-inference:latest (TGI alternative)
SERVING_RUNTIME_IMAGE=python:3.11-slim

# Use KServe InferenceService (requires Knative + Istio)
# Set to false if KServe is not properly installed
# Values: true, false
USE_KSERVE=false

# KServe controller namespace
KSERVE_NAMESPACE=kserve

# Request GPU resources for serving (set to false for CPU-only)
# Values: true, false
USE_GPU=false

# =============================================================================
# Serving Resource Limits (GPU-enabled)
# =============================================================================
# CPU and memory requests/limits when GPU is enabled
# Format: "1" (1 core), "500m" (0.5 core), "2Gi" (2 gibibytes), "512Mi" (512 mebibytes)
# Adjust based on your cluster capacity

SERVING_CPU_REQUEST=1
SERVING_CPU_LIMIT=2
SERVING_MEMORY_REQUEST=2Gi
SERVING_MEMORY_LIMIT=4Gi

# =============================================================================
# Serving Resource Limits (CPU-only)
# =============================================================================
# CPU and memory requests/limits when GPU is disabled
# Use smaller values for local development

SERVING_CPU_ONLY_CPU_REQUEST=1
SERVING_CPU_ONLY_CPU_LIMIT=2
SERVING_CPU_ONLY_MEMORY_REQUEST=1Gi
SERVING_CPU_ONLY_MEMORY_LIMIT=2Gi

