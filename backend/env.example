# =============================================================================
# LLM Ops Platform - Environment Configuration Example
# =============================================================================
# Copy this file to .env and adjust values for your environment:
#   cd backend
#   cp env.example .env
#
# IMPORTANT: 
#   - Do not commit .env file to git (it's in .gitignore)
#   - All boolean values should be lowercase: true or false
#   - Empty values for optional fields can be left as empty string or commented out

# =============================================================================
# Database Configuration
# =============================================================================
# PostgreSQL connection URL
# Format: postgresql+psycopg://user:password@host:port/database
# Local (with port-forward): postgresql+psycopg://llmops:password@localhost:5432/llmops
# Kubernetes: postgresql+psycopg://llmops:password@postgresql.llm-ops-dev.svc.cluster.local:5432/llmops
DATABASE_URL=postgresql+psycopg://llmops:password@localhost:5432/llmops

# Enable SQLAlchemy query logging (for debugging)
# Values: true, false
SQLALCHEMY_ECHO=false

# =============================================================================
# Redis Configuration
# =============================================================================
# Redis connection URL
# Local (with port-forward): redis://localhost:6379/0
# Kubernetes: redis://redis.llm-ops-dev.svc.cluster.local:6379/0
REDIS_URL=redis://localhost:6379/0

# =============================================================================
# Object Storage Configuration (MinIO/S3)
# =============================================================================
# Object storage endpoint URL
# Local (with port-forward): http://localhost:9000
# Kubernetes: http://minio.llm-ops-dev.svc.cluster.local:9000
# Production S3: https://s3.amazonaws.com
OBJECT_STORE_ENDPOINT=http://localhost:9000

# Object storage access credentials
OBJECT_STORE_ACCESS_KEY=llmops
OBJECT_STORE_SECRET_KEY=llmops-secret

# Use HTTPS for object storage (set to true for production S3)
# Values: true, false
OBJECT_STORE_SECURE=false

# =============================================================================
# Application Configuration
# =============================================================================
# Prometheus metrics namespace
PROMETHEUS_NAMESPACE=llm_ops

# Default required role for API access
DEFAULT_REQUIRED_ROLE=llm-ops-user

# =============================================================================
# Kubernetes Configuration
# =============================================================================
# Path to kubeconfig file (leave empty or unset to use default or in-cluster config)
# Local: ~/.kube/config or /path/to/kubeconfig
# In-cluster: leave this line commented out or empty (auto-detected)
# Uncomment and set if you need a specific kubeconfig file:
# KUBECONFIG_PATH=~/.kube/config

# =============================================================================
# Training Image Configuration (training-serving-spec.md compliant)
# =============================================================================
# Container images for different training job types
# Each job type supports GPU and CPU variants
# Format: TRAIN_IMAGE_{JOB_TYPE}_{GPU|CPU}=<image:tag>
#
# Job types: PRETRAIN, SFT, RAG_TUNING, RLHF, EMBEDDING
# 
# Examples:
#   - PyTorch-based: pytorch/pytorch:2.1.0-cuda12.1-cudnn8-runtime
#   - Custom registry: registry.example.com/llm-train-sft:pytorch2.1-cuda12.1-v1
#   - CPU-only: pytorch/pytorch:2.1.0-cpu

# PRETRAIN training images
TRAIN_IMAGE_PRETRAIN_GPU=registry/llm-train-pretrain:pytorch2.1-cuda12.1-v1
TRAIN_IMAGE_PRETRAIN_CPU=registry/llm-train-pretrain:pytorch2.1-cpu-v1

# SFT (Supervised Fine-tuning) training images
TRAIN_IMAGE_SFT_GPU=registry/llm-train-sft:pytorch2.1-cuda12.1-v1
TRAIN_IMAGE_SFT_CPU=registry/llm-train-sft:pytorch2.1-cpu-v1

# RAG_TUNING training images
TRAIN_IMAGE_RAG_TUNING_GPU=registry/llm-train-rag:pytorch2.1-cuda12.1-v1
TRAIN_IMAGE_RAG_TUNING_CPU=registry/llm-train-rag:pytorch2.1-cpu-v1

# RLHF (Reinforcement Learning from Human Feedback) training images
TRAIN_IMAGE_RLHF_GPU=registry/llm-train-rlhf:pytorch2.1-cuda12.1-v1
TRAIN_IMAGE_RLHF_CPU=registry/llm-train-rlhf:pytorch2.1-cpu-v1

# EMBEDDING training images
TRAIN_IMAGE_EMBEDDING_GPU=registry/llm-train-embedding:pytorch2.1-cuda12.1-v1
TRAIN_IMAGE_EMBEDDING_CPU=registry/llm-train-embedding:pytorch2.1-cpu-v1

# =============================================================================
# Serving Image Configuration (training-serving-spec.md compliant)
# =============================================================================
# Container images for different serving targets
# Each serve target supports GPU and CPU variants
# Format: SERVE_IMAGE_{SERVE_TARGET}_{GPU|CPU}=<image:tag>
#
# Serve targets: GENERATION, RAG
#
# Examples for GENERATION:
#   - vLLM: ghcr.io/vllm/vllm:latest or vllm/vllm-openai:nightly
#   - TGI: ghcr.io/huggingface/text-generation-inference:latest
#   - Custom: registry.example.com/llm-serve:vllm-0.5.0-cuda12.1
#
# Examples for RAG:
#   - Custom RAG runtime: registry.example.com/llm-serve-rag:vllm-0.5.0-cuda12.1

# GENERATION serving images (for SFT/RLHF/PRETRAIN models)
SERVE_IMAGE_GENERATION_GPU=registry/llm-serve:vllm-0.5.0-cuda12.1
SERVE_IMAGE_GENERATION_CPU=registry/llm-serve:cpu-v0.5.0

# RAG serving images (for RAG_TUNING models)
SERVE_IMAGE_RAG_GPU=registry/llm-serve-rag:vllm-0.5.0-cuda12.1
SERVE_IMAGE_RAG_CPU=registry/llm-serve-rag:cpu-v0.5.0

# =============================================================================
# Legacy Serving Configuration (deprecated, use SERVE_IMAGE_* above)
# =============================================================================
# Model serving runtime image (fallback if SERVE_IMAGE_* not set)
# Options:
#   - python:3.11-slim (lightweight, for local testing)
#   - ghcr.io/vllm/vllm:latest (GitHub Container Registry)
#   - ghcr.io/huggingface/text-generation-inference:latest (TGI)
SERVING_RUNTIME_IMAGE=python:3.11-slim

# =============================================================================
# Serving Framework Configuration
# =============================================================================
# Use KServe InferenceService (requires Knative + Istio)
# Set to false if KServe is not properly installed
# Values: true, false
# Default: false for minimum requirements (CPU-only development)
USE_KSERVE=false

# KServe controller namespace
KSERVE_NAMESPACE=kserve

# Request GPU resources for serving (set to false for CPU-only)
# Values: true, false
# Default: false for minimum requirements (CPU-only development)
USE_GPU=false

# =============================================================================
# Serving Resource Limits (GPU-enabled)
# =============================================================================
# CPU and memory requests/limits when GPU is enabled
# Format: "1" (1 core), "500m" (0.5 core), "2Gi" (2 gibibytes), "512Mi" (512 mebibytes)
# Adjust based on your cluster capacity

SERVING_CPU_REQUEST=1
SERVING_CPU_LIMIT=2
SERVING_MEMORY_REQUEST=2Gi
SERVING_MEMORY_LIMIT=4Gi

# =============================================================================
# Serving Resource Limits (CPU-only)
# =============================================================================
# CPU and memory requests/limits when GPU is disabled
# Minimum requirements for local development (CPU-only mode)
# These values are optimized for minimum resource usage

SERVING_CPU_ONLY_CPU_REQUEST=500m
SERVING_CPU_ONLY_CPU_LIMIT=1
SERVING_CPU_ONLY_MEMORY_REQUEST=512Mi
SERVING_CPU_ONLY_MEMORY_LIMIT=1Gi

# =============================================================================
# Open Source Integration Configuration
# =============================================================================
# Enable model registry integration (Hugging Face Hub, etc.)
# Values: true, false
MODEL_REGISTRY_ENABLED=true

# Hugging Face Hub token (optional, for private repos)
# Leave empty or unset for public repos only
# HUGGINGFACE_HUB_TOKEN=your_token_here

# Hugging Face Hub cache directory
# Default: /tmp/hf_cache
# HUGGINGFACE_HUB_CACHE_DIR=/tmp/hf_cache

# Maximum model size in GB that can be imported from Hugging Face Hub
# Set to 0 or negative value to disable size limit
# Default: 5.0 GB
# HUGGINGFACE_MAX_DOWNLOAD_SIZE_GB=5.0

