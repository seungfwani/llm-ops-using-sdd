# =============================================================================
# LLM Ops Platform - Environment Configuration Example
# =============================================================================
# Copy this file to .env and adjust values for your environment:
#   cd backend
#   cp env.example .env
#
# IMPORTANT: 
#   - Do not commit .env file to git (it's in .gitignore)
#   - All boolean values should be lowercase: true or false
#   - Empty values for optional fields can be left as empty string or commented out

# =============================================================================
# Database Configuration
# =============================================================================
# PostgreSQL connection URL (OPTIONAL - can be replaced by individual components below)
# Format: postgresql+psycopg://user:password@host:port/database
# Local (with port-forward): postgresql+psycopg://llmops:password@localhost:5432/llmops
# Kubernetes: postgresql+psycopg://llmops:password@postgresql.llm-ops-dev.svc.cluster.local:5432/llmops
# If not set, will be constructed from DB_USER, DB_PASSWORD, DB_HOST, DB_PORT, DB_NAME below
# DATABASE_URL=postgresql+psycopg://llmops:password@localhost:5432/llmops

# PostgreSQL connection components (used when DATABASE_URL is not set)
# These are typically loaded from Kubernetes secrets via secretKeyRef in production
# Local development: Set these values directly
# Kubernetes: Values are injected from postgresql-secret via secretKeyRef
DB_USER=llmops
DB_PASSWORD=password
DB_HOST=localhost
DB_PORT=5432
DB_NAME=llmops

# Enable SQLAlchemy query logging (for debugging)
# Values: true, false
SQLALCHEMY_ECHO=false

# =============================================================================
# Redis Configuration
# =============================================================================
# Redis connection URL (OPTIONAL - can be replaced by individual components below)
# Format: redis://host:port/db or redis://:password@host:port/db
# Local (with port-forward): redis://localhost:6379/0
# Kubernetes: redis://redis.llm-ops-dev.svc.cluster.local:6379/0
# With password: redis://:password@localhost:6379/0
# If not set, will be constructed from REDIS_HOST, REDIS_PORT, REDIS_DATABASE, REDIS_PASSWORD below
# REDIS_URL=redis://localhost:6379/0

# Redis connection components (used when REDIS_URL is not set)
# These are typically loaded from Kubernetes secrets via secretKeyRef in production (if password enabled)
# Local development: Set these values directly
# Kubernetes: REDIS_PASSWORD is injected from redis-secret via secretKeyRef (if enabled)
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DATABASE=0
# Redis password (optional - only needed if Redis authentication is enabled)
# REDIS_PASSWORD=

# =============================================================================
# Object Storage Configuration (MinIO/S3)
# =============================================================================
# Object storage endpoint URL
# Local (with port-forward): http://localhost:9000
# Kubernetes: http://minio.llm-ops-dev.svc.cluster.local:9000
# Production S3: https://s3.amazonaws.com
OBJECT_STORE_ENDPOINT=http://localhost:9000

# Object storage access credentials
OBJECT_STORE_ACCESS_KEY=llmops
OBJECT_STORE_SECRET_KEY=llmops-secret

# Use HTTPS for object storage (set to true for production S3)
# Values: true, false
OBJECT_STORE_SECURE=false

# Object storage bucket name (optional; defaults to training namespace)
# Format: llm-ops-{namespace} (e.g., llm-ops-dev, llm-ops-stg, llm-ops-prod)
# OBJECT_STORE_BUCKET=llm-ops-dev

# =============================================================================
# Application Configuration
# =============================================================================
# Prometheus metrics namespace
PROMETHEUS_NAMESPACE=llm_ops

# Default required role for API access
DEFAULT_REQUIRED_ROLE=llm-ops-user

# Logging level for application logs
# Values: DEBUG, INFO, WARNING, ERROR, CRITICAL
# Default: INFO
# DEBUG: Detailed debug information (useful for development)
# INFO: General informational messages (recommended for production)
# WARNING: Warning messages only
# ERROR: Error messages only
# CRITICAL: Critical errors only
LOG_LEVEL=INFO

# =============================================================================
# Kubernetes Configuration
# =============================================================================
# Path to kubeconfig file (leave empty or unset to use default or in-cluster config)
# Local: ~/.kube/config or /path/to/kubeconfig
# In-cluster: leave this line commented out or empty (auto-detected)
# Uncomment and set if you need a specific kubeconfig file:
# KUBECONFIG_PATH=~/.kube/config

# Disable SSL verification for Kubernetes API (for self-signed certificates)
# WARNING: Only use this in development/testing environments
# Set to false to disable SSL certificate verification (not recommended for production)
# Values: true, false
# Default: true
KUBERNETES_VERIFY_SSL=true

# =============================================================================
# Training Image Configuration (training-serving-spec.md compliant)
# =============================================================================
# Container images for different training job types
# Each job type supports GPU and CPU variants
# Format: TRAIN_IMAGE_{JOB_TYPE}_{GPU|CPU}=<image:tag>
#
# Job types: PRETRAIN, SFT, RAG_TUNING, RLHF, EMBEDDING
# 
# Examples:
#   - PyTorch-based: pytorch/pytorch:2.1.0-cuda12.1-cudnn8-runtime
#   - Custom registry: registry.example.com/llm-train-sft:pytorch2.1-cuda12.1-v1
#   - CPU-only: pytorch/pytorch:2.1.0-cpu

# PRETRAIN training images
TRAIN_IMAGE_PRETRAIN_GPU=registry/llm-train-pretrain:pytorch2.1-cuda12.1-v1
TRAIN_IMAGE_PRETRAIN_CPU=registry/llm-train-pretrain:pytorch2.1-cpu-v1

# SFT (Supervised Fine-tuning) training images
TRAIN_IMAGE_SFT_GPU=registry/llm-train-sft:pytorch2.1-cuda12.1-v1
TRAIN_IMAGE_SFT_CPU=registry/llm-train-sft:pytorch2.1-cpu-v1

# RAG_TUNING training images
TRAIN_IMAGE_RAG_TUNING_GPU=registry/llm-train-rag:pytorch2.1-cuda12.1-v1
TRAIN_IMAGE_RAG_TUNING_CPU=registry/llm-train-rag:pytorch2.1-cpu-v1

# RLHF (Reinforcement Learning from Human Feedback) training images
TRAIN_IMAGE_RLHF_GPU=registry/llm-train-rlhf:pytorch2.1-cuda12.1-v1
TRAIN_IMAGE_RLHF_CPU=registry/llm-train-rlhf:pytorch2.1-cpu-v1

# EMBEDDING training images
TRAIN_IMAGE_EMBEDDING_GPU=registry/llm-train-embedding:pytorch2.1-cuda12.1-v1
TRAIN_IMAGE_EMBEDDING_CPU=registry/llm-train-embedding:pytorch2.1-cpu-v1

# =============================================================================
# Training GPU Type Configuration
# =============================================================================
# Comma-separated GPU type IDs per environment (used for validation and UI options)
# Example: nvidia-rtx-4090,nvidia-rtx-a6000
# You can provide as comma-separated or JSON list, e.g.:
# TRAINING_GPU_TYPES_DEV=nvidia-rtx-4090,nvidia-rtx-a6000
# TRAINING_GPU_TYPES_DEV=["nvidia-rtx-4090","nvidia-rtx-a6000"]
TRAINING_GPU_TYPES_DEV=nvidia-rtx-4090,nvidia-rtx-a6000
TRAINING_GPU_TYPES_STG=
TRAINING_GPU_TYPES_PROD=

# =============================================================================
# Training Resource Limits (CPU-only)
# =============================================================================
# CPU and memory requests/limits for CPU-only training jobs
# Format: "4" (4 cores), "8Gi" (8 gibibytes)
TRAINING_CPU_ONLY_CPU_REQUEST=2
TRAINING_CPU_ONLY_CPU_LIMIT=4
TRAINING_CPU_ONLY_MEMORY_REQUEST=2Gi
TRAINING_CPU_ONLY_MEMORY_LIMIT=4Gi

# =============================================================================
# Training Resource Limits (GPU-enabled)
# =============================================================================
# CPU and memory requests/limits for GPU training jobs
TRAINING_GPU_CPU_REQUEST=4
TRAINING_GPU_CPU_LIMIT=8
TRAINING_GPU_MEMORY_REQUEST=4Gi
TRAINING_GPU_MEMORY_LIMIT=8Gi

# Distributed training memory overrides (per pod)
TRAINING_GPU_DISTRIBUTED_MEMORY_REQUEST=16Gi
TRAINING_GPU_DISTRIBUTED_MEMORY_LIMIT=32Gi

# =============================================================================
# Training Configuration
# =============================================================================
# Kubernetes namespace for training jobs (format: llm-ops-{environment})
TRAINING_NAMESPACE=llm-ops-dev

# GPU node selector (JSON or empty for any node), e.g. {"node-type":"gpu"}
# TRAINING_GPU_NODE_SELECTOR={}

# GPU node tolerations (JSON list), e.g. [{"key":"nvidia.com/gpu","operator":"Exists","effect":"NoSchedule"}]
# TRAINING_GPU_TOLERATIONS=[]

# API base URL for training pods to record metrics (leave empty to disable)
TRAINING_API_BASE_URL=

# Interval (seconds) to sync training job status; 0 disables auto sync
TRAINING_JOB_STATUS_SYNC_INTERVAL=30

# =============================================================================
# Serving Image Configuration (training-serving-spec.md compliant)
# =============================================================================
# Container images for different serving targets
# Each serve target supports GPU and CPU variants
# Format: SERVE_IMAGE_{SERVE_TARGET}_{GPU|CPU}=<image:tag>
#
# Serve targets: GENERATION, RAG
#
# Examples for GENERATION:
#   - vLLM: ghcr.io/vllm/vllm:latest or vllm/vllm-openai:nightly
#   - TGI: ghcr.io/huggingface/text-generation-inference:latest
#   - Custom: registry.example.com/llm-serve:vllm-0.5.0-cuda12.1
#
# Examples for RAG:
#   - Custom RAG runtime: registry.example.com/llm-serve-rag:vllm-0.5.0-cuda12.1

# GENERATION serving images (for SFT/RLHF/PRETRAIN models)
# TGI is recommended for HuggingFace models, vLLM for others
SERVE_IMAGE_GENERATION_GPU=ghcr.io/huggingface/text-generation-inference:latest
SERVE_IMAGE_GENERATION_CPU=ghcr.io/huggingface/text-generation-inference:latest

# RAG serving images (for RAG_TUNING models)
# vLLM is commonly used for RAG workloads
SERVE_IMAGE_RAG_GPU=ghcr.io/vllm/vllm:latest
SERVE_IMAGE_RAG_CPU=ghcr.io/vllm/vllm:latest

# =============================================================================
# Legacy Serving Configuration (deprecated, use SERVE_IMAGE_* above)
# =============================================================================
# Model serving runtime image (fallback if SERVE_IMAGE_* not set)
# NOTE: This is only used as a last resort fallback. The platform automatically
# selects appropriate images based on:
#   - DeploymentSpec.serve_target (GENERATION or RAG)
#   - Model metadata (huggingface_model_id for HuggingFace models)
#   - GPU availability (use_gpu flag)
# 
# Options:
#   - ghcr.io/huggingface/text-generation-inference:latest (TGI, for HuggingFace models)
#   - ghcr.io/vllm/vllm:latest (vLLM, for other models)
#   - python:3.11-slim (NOT RECOMMENDED - only for custom runtime development)
# 
# For proper image selection, configure SERVE_IMAGE_GENERATION_GPU/CPU and SERVE_IMAGE_RAG_GPU/CPU
# environment variables instead. See image_config.py for details.
SERVING_RUNTIME_IMAGE=ghcr.io/huggingface/text-generation-inference:latest

# Optional override for local development: if set, internal inference calls
# will use this base URL instead of Kubernetes cluster DNS.
# Example: http://localhost:8001 (port-forwarded serving service)
# SERVING_LOCAL_BASE_URL=http://localhost:8001

# Optional override when Kubernetes DNS hostnames are unreachable (e.g., external
# node IP access). If set, inference calls will use this base host/IP directly.
# Example: http://10.0.0.5:8000
# SERVING_INFERENCE_HOST_OVERRIDE=http://10.0.0.5:8000

# =============================================================================
# Serving Framework Configuration
# =============================================================================
# Use KServe InferenceService (requires Knative + Istio)
# Set to false if KServe is not properly installed
# Values: true, false
# Default: false for minimum requirements (CPU-only development)
USE_KSERVE=false

# KServe controller namespace
KSERVE_NAMESPACE=kserve

# Request GPU resources for serving (set to false for CPU-only)
# Values: true, false
# Default: false for minimum requirements (CPU-only development)
USE_GPU=false

# =============================================================================
# Serving Resource Limits (GPU-enabled)
# =============================================================================
# CPU and memory requests/limits when GPU is enabled
# Format: "1" (1 core), "500m" (0.5 core), "2Gi" (2 gibibytes), "512Mi" (512 mebibytes)
# Adjust based on your cluster capacity

SERVING_CPU_REQUEST=1
SERVING_CPU_LIMIT=2
SERVING_MEMORY_REQUEST=2Gi
SERVING_MEMORY_LIMIT=16Gi

# =============================================================================
# Serving Resource Limits (CPU-only)
# =============================================================================
# CPU and memory requests/limits when GPU is disabled
# Minimum requirements for local development (CPU-only mode)
# These values are optimized for minimum resource usage

SERVING_CPU_ONLY_CPU_REQUEST=500m
SERVING_CPU_ONLY_CPU_LIMIT=1
SERVING_CPU_ONLY_MEMORY_REQUEST=512Mi
SERVING_CPU_ONLY_MEMORY_LIMIT=1Gi

# =============================================================================
# Feature Flags & Integrations
# =============================================================================
# Experiment tracking integration
EXPERIMENT_TRACKING_ENABLED=false
EXPERIMENT_TRACKING_SYSTEM=mlflow

# Serving framework integration (kserve, ray_serve, etc.)
SERVING_FRAMEWORK_ENABLED=false
SERVING_FRAMEWORK_DEFAULT=kserve

# Workflow orchestration integration (argo_workflows, kubeflow, etc.)
WORKFLOW_ORCHESTRATION_ENABLED=false
WORKFLOW_ORCHESTRATION_SYSTEM=argo_workflows

# Model registry integration
MODEL_REGISTRY_ENABLED=true
MODEL_REGISTRY_DEFAULT=huggingface

# Data versioning integration
DATA_VERSIONING_ENABLED=false
DATA_VERSIONING_SYSTEM=dvc

# Environment identifier (dev, stg, prod)
ENVIRONMENT=dev

# =============================================================================
# MLflow Configuration
# =============================================================================
# Enable MLflow integration
MLFLOW_ENABLED=false

# MLflow tracking URI (e.g., http://mlflow-service.mlflow.svc.cluster.local:5000)
# MLFLOW_TRACKING_URI=

# MLflow backend store URI (e.g., PostgreSQL URI)
# MLFLOW_BACKEND_STORE_URI=

# MLflow default artifact root (e.g., s3://mlflow-artifacts)
# MLFLOW_DEFAULT_ARTIFACT_ROOT=

# =============================================================================
# Argo Workflows Configuration
# =============================================================================
ARGO_WORKFLOWS_ENABLED=false
ARGO_WORKFLOWS_NAMESPACE=argo
ARGO_WORKFLOWS_CONTROLLER_SERVICE=argo-workflows-server.argo.svc.cluster.local:2746

# =============================================================================
# Hugging Face Hub Configuration
# =============================================================================
HUGGINGFACE_HUB_ENABLED=false
# Hugging Face Hub token (optional, for private repos)
# HUGGINGFACE_HUB_TOKEN=your_token_here
# Hugging Face Hub cache directory
# HUGGINGFACE_HUB_CACHE_DIR=/tmp/hf_cache
# Maximum model size in GB that can be imported from Hugging Face Hub
# HUGGINGFACE_MAX_DOWNLOAD_SIZE_GB=5.0
# Number of concurrent uploads to MinIO/S3 when importing models
# Higher values increase upload speed but may consume more resources
# Recommended: 5-10 for most cases, adjust based on network bandwidth and server resources
# Default: 5 concurrent uploads
# HUGGINGFACE_CONCURRENT_UPLOADS=5

# =============================================================================
# DVC Configuration
# =============================================================================
DVC_ENABLED=false
DVC_REMOTE_NAME=minio
# DVC remote URL (e.g., s3://datasets-dvc)
# DVC_REMOTE_URL=
DVC_CACHE_DIR=/tmp/dvc-cache

