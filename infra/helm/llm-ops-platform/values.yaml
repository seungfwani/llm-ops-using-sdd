kserve:
  enabled: true
  namespaceOverride: ""
  version: v0.16.0
  setDefaultDeploymentMode: true
  selfSignedCert:
    enabled: true
    serviceName: kserve-webhook-server-service
    secretName: kserve-webhook-server-cert
    image: bitnami/kubectl:latest

gpuPlugin:
  enabled: true

nvidia-device-plugin:
  namespaceOverride: kube-system

  config:
    # ConfigMap 안에서 디폴트로 사용할 config 이름
    default: config.yaml

    # 여기부터가 실제 config 내용 (파일명: 내용)
    map:
      config.yaml: |-
        version: v1
        sharing:
          timeSlicing:
            resources:
              - name: nvidia.com/gpu
                replicas: 10

  args:
    - --fail-on-init-error=false
    - --config-file=/config/config.yaml

dependencies:
  postgresql:
    enabled: true
    storage: 5Gi
    image: postgres:16-alpine
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 250m
        memory: 256Mi
    service:
      type: NodePort
      port: 5432
      nodePort: 30001
    secret:
      user: llmops
      password: password
      db: llmops
  redis:
    enabled: true
    image: redis:7-alpine
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 200m
        memory: 256Mi
    service:
      type: ClusterIP
      port: 6379
    secret:
      enabled: false  # Set to true to enable Redis password authentication
      password: ""  # Redis password (required if enabled is true)
  minio:
    enabled: true
    storage: 50Gi
    image: minio/minio:latest
    resources:
      requests:
        cpu: 100m
        memory: 256Mi
      limits:
        cpu: 250m
        memory: 512Mi
    service:
      type: NodePort
      ports:
        api: 9000
        console: 9001
      nodePorts:
        api: 30002
        console: 30003
    secret:
      accessKey: llmops
      secretKey: llmops-secret

objectStore:
  # bucket이 비어있으면 자동으로 네임스페이스 이름을 사용합니다
  bucket: ""
  endpoint: http://minio:9000
  createBucket: true
  jobImage: quay.io/minio/mc:latest
  credentials:
    accessKey: llmops
    secretKey: llmops-secret
  config:
    endpointUrl: http://minio:9000

minioBucketJob:
  enabled: true

# ServiceAccount and RBAC for backend API server
# Required for Kubernetes API access (deploying InferenceServices, Deployments, etc.)
serviceAccount:
  create: true
  # If not set, uses the default service account name from _helpers.tpl
  # name: ""

rbac:
  create: true
  # ClusterRole and ClusterRoleBinding will be created with permissions
  # to manage Deployments, Services, Ingresses, HPA, and KServe InferenceServices

# Application (Frontend + Backend) Configuration
app:
  enabled: true
  replicas: 1
  
  image:
    repository: llm-ops-platform
    tag: "0.1.0"
    pullPolicy: IfNotPresent
  
  service:
    type: ClusterIP
    port: 8000
    # nodePort: 30000  # Uncomment and set if type is NodePort
  
  # Database connection URL
  # Format: postgresql+psycopg://user:password@host:port/dbname
  # If not set, will be auto-generated from dependencies.postgresql template
  # databaseUrl: ""  # Leave empty to auto-generate from dependencies
  
  # Database service configuration
  # If databaseUrl is not set, individual components (DB_USER, DB_PASSWORD, etc.) will be used
  # When dependencies.postgresql.enabled is true, credentials are loaded from postgresql-secret via secretKeyRef
  # When dependencies.postgresql.enabled is false, fallback values below are used
  database:
    serviceName: "postgresql"  # Service name from postgresql template (line 85)
    port: 5432
    # Fallback values (only used when dependencies.postgresql.enabled is false)
    # If not set, defaults to "llmops", "password", "llmops"
    # user: ""
    # password: ""
    # database: ""
  
  # Redis connection URL
  # Format: redis://host:port/db or redis://:password@host:port/db
  # If not set, will be auto-generated from individual components (REDIS_HOST, REDIS_PORT, etc.)
  # redisUrl: ""  # Leave empty to auto-generate from components
  
  # Redis service configuration (used to build REDIS_URL components if redisUrl not set)
  # When dependencies.redis.secret.enabled is true, password is loaded from redis-secret via secretKeyRef
  redis:
    serviceName: "redis"  # Service name from redis template (line 47)
    port: 6379
    database: 0
  
  # Object store endpoint
  # Format: http://host:port or https://host:port
  # If not set, will be auto-generated from dependencies.minio template
  # objectStoreEndpoint: ""  # Leave empty to auto-generate from dependencies
  
  # Object store service configuration (used to build OBJECT_STORE_ENDPOINT if objectStoreEndpoint not set)
  objectStoreService:
    serviceName: "minio"  # Service name from minio template (line 85)
    port: 9000
    secure: false  # Use https if true
  
  # Application settings
  useKserve: true
  useGpu: false
  logLevel: "INFO"
  
  # Database Configuration
  sqlalchemyEcho: false
  
  # Object Storage Configuration
  objectStoreSecure: false
  objectStoreBucket: ""  # Optional, defaults to namespace
  
  # Application Configuration
  prometheusNamespace: "llm_ops"
  defaultRequiredRole: "llm-ops-user"
  
  # Kubernetes Configuration
  kubeconfigPath: ""  # Empty for in-cluster config
  kubernetesVerifySsl: true
  
  # Training Image Configuration
  trainingImages:
    pretrain:
      gpu: "registry/llm-train-pretrain:pytorch2.1-cuda12.1-v1"
      cpu: "registry/llm-train-pretrain:pytorch2.1-cpu-v1"
    sft:
      gpu: "registry/llm-train-sft:pytorch2.1-cuda12.1-v1"
      cpu: "registry/llm-train-sft:pytorch2.1-cpu-v1"
    ragTuning:
      gpu: "registry/llm-train-rag:pytorch2.1-cuda12.1-v1"
      cpu: "registry/llm-train-rag:pytorch2.1-cpu-v1"
    rlhf:
      gpu: "registry/llm-train-rlhf:pytorch2.1-cuda12.1-v1"
      cpu: "registry/llm-train-rlhf:pytorch2.1-cpu-v1"
    embedding:
      gpu: "registry/llm-train-embedding:pytorch2.1-cuda12.1-v1"
      cpu: "registry/llm-train-embedding:pytorch2.1-cpu-v1"
  
  # Training GPU Type Configuration
  trainingGpuTypes:
    dev: "nvidia-rtx-4090,nvidia-rtx-a6000"
    stg: ""
    prod: ""
  
  # Training Resource Limits (CPU-only)
  trainingCpuOnly:
    cpuRequest: "2"
    cpuLimit: "4"
    memoryRequest: "2Gi"
    memoryLimit: "4Gi"
  
  # Training Resource Limits (GPU-enabled)
  trainingGpu:
    cpuRequest: "4"
    cpuLimit: "8"
    memoryRequest: "4Gi"
    memoryLimit: "8Gi"
    distributedMemoryRequest: "16Gi"
    distributedMemoryLimit: "32Gi"
  
  # Training Configuration
  trainingGpuNodeSelector: {}  # JSON object, e.g. {"node-type":"gpu"}
  trainingGpuTolerations: []  # JSON array, e.g. [{"key":"nvidia.com/gpu","operator":"Exists","effect":"NoSchedule"}]
  trainingApiBaseUrl: ""  # Empty to disable
  trainingJobStatusSyncInterval: 30  # seconds, 0 to disable
  
  # Serving Image Configuration
  servingImages:
    generation:
      gpu: "ghcr.io/huggingface/text-generation-inference:latest"
      cpu: "ghcr.io/huggingface/text-generation-inference:latest"
    rag:
      gpu: "ghcr.io/vllm/vllm:latest"
      cpu: "ghcr.io/vllm/vllm:latest"
  
  # Legacy Serving Configuration (fallback)
  servingRuntimeImage: "ghcr.io/huggingface/text-generation-inference:latest"
  servingLocalBaseUrl: ""  # Optional override for local development
  servingInferenceHostOverride: ""  # Optional override for external IP access
  
  # Serving Resource Limits (GPU-enabled)
  servingGpu:
    cpuRequest: "1"
    cpuLimit: "2"
    memoryRequest: "2Gi"
    memoryLimit: "16Gi"
  
  # Serving Resource Limits (CPU-only)
  servingCpuOnly:
    cpuRequest: "500m"
    cpuLimit: "1"
    memoryRequest: "512Mi"
    memoryLimit: "1Gi"
  
  # Feature Flags & Integrations
  experimentTracking:
    enabled: false
    system: "mlflow"
  
  servingFramework:
    enabled: false
    default: "kserve"
  
  workflowOrchestration:
    enabled: false
    system: "argo_workflows"
  
  modelRegistry:
    enabled: true
    default: "huggingface"
  
  dataVersioning:
    enabled: false
    system: "dvc"
  
  environment: "dev"  # dev, stg, prod
  
  # MLflow Configuration
  mlflow:
    enabled: false
    # trackingUri: ""  # e.g., http://mlflow-service.mlflow.svc.cluster.local:5000
    # backendStoreUri: ""  # PostgreSQL URI
    # defaultArtifactRoot: ""  # S3 URI
  
  # Argo Workflows Configuration
  argoWorkflows:
    enabled: false
    namespace: "argo"
    controllerService: "argo-workflows-server.argo.svc.cluster.local:2746"
  
  # Hugging Face Hub Configuration
  huggingfaceHub:
    enabled: false
    # token: ""  # Optional, for private repos
    cacheDir: "/tmp/hf_cache"
    maxDownloadSizeGb: 5.0
    concurrentUploads: 5
  
  # DVC Configuration
  dvc:
    enabled: false
    remoteName: "minio"
    # remoteUrl: ""  # e.g., s3://datasets-dvc
    cacheDir: "/tmp/dvc-cache"
  
  # Resource limits
  resources:
    requests:
      cpu: 500m
      memory: 512Mi
    limits:
      cpu: 2000m
      memory: 2Gi
  
  # Health check probes
  livenessProbe:
    httpGet:
      path: /llm-ops/v1/health
      port: http
    initialDelaySeconds: 30
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 3
  
  readinessProbe:
    httpGet:
      path: /llm-ops/v1/health
      port: http
    initialDelaySeconds: 10
    periodSeconds: 5
    timeoutSeconds: 3
    failureThreshold: 3
  
  # Optional: Override service account name
  # serviceAccountName: ""
  
  # Optional: Node selector for pod placement
  # nodeSelector: {}
  
  # Optional: Tolerations for tainted nodes
  # tolerations: []
  
  # Optional: Affinity rules
  # affinity: {}
  
  # Optional: Additional environment variables
  # extraEnv: []
